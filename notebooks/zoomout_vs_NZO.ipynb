{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example notebook on how to use NAM for shape matching\n",
    "In this notebook we consider different refinement with the spectral embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"pytorch\"\n",
    "import geomstats.backend as gs\n",
    "from geomfum.shape.mesh import TriangleMesh\n",
    "from geomfum.refine import ZoomOut\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from model.neural_adjoint_map import NeuralAdjointMap\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from model.neural_zoomout import NeuralZoomOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "faust_url = \"https://raw.githubusercontent.com/JM-data/PyFuncMap/4bde4484c3e93bff925a6a82da29fa79d6862f4b/FAUST_shapes_off/\"\n",
    "shape_files = [\"tr_reg_080.off\", \"tr_reg_093.off\"]\n",
    "for fname in shape_files:\n",
    "    url = faust_url + fname\n",
    "    out_path = os.path.join(\"../data/\", fname)\n",
    "    urlretrieve(url, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuli\\OneDrive\\Research\\NAM-NeuralAdjointMaps\\venv\\Lib\\site-packages\\geomfum\\_backend\\pytorch\\sparse.py:22: UserWarning: Sparse CSC tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  return _torch.sparse_csc_tensor(ccol_indices, row_indices, values, size=array.shape)\n"
     ]
    }
   ],
   "source": [
    "mesh1 = TriangleMesh.from_file(\"../data/tr_reg_080.off\")\n",
    "mesh2 = TriangleMesh.from_file(\"../data/tr_reg_093.off\")\n",
    "\n",
    "eigvals1, eigvecs1 = mesh1.laplacian.find_spectrum(spectrum_size=200)\n",
    "eigvals2, eigvecs2 = mesh2.laplacian.find_spectrum(spectrum_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_gt = np.arange(mesh1.n_vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Error: tensor(0.0011)\n"
     ]
    }
   ],
   "source": [
    "from geomfum.convert import FmFromP2pConverter, P2pFromFmConverter\n",
    "\n",
    "\n",
    "fmap_from_p2p=FmFromP2pConverter()\n",
    "p2p_from_fmap = P2pFromFmConverter()\n",
    "\n",
    "mesh1.basis.use_k=20\n",
    "mesh2.basis.use_k=20\n",
    "fmap = fmap_from_p2p(p2p_gt, mesh1.basis, mesh2.basis)\n",
    "\n",
    "\n",
    "p2p_ini = p2p_from_fmap(fmap, mesh1.basis, mesh2.basis)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Conversion Error:\",\n",
    "    gs.mean((mesh2.vertices[p2p_ini] - mesh2.vertices[p2p_gt]) ** 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZoomOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh1.basis.use_k=200\n",
    "mesh2.basis.use_k=200\n",
    "zoomout = ZoomOut(nit=9, step=20)\n",
    "ref_fmap = zoomout(fmap, mesh1.basis, mesh2.basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Error: tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "p2p = p2p_from_fmap(ref_fmap, mesh1.basis, mesh2.basis)\n",
    "\n",
    "print(\n",
    "    \"Conversion Error:\",\n",
    "    gs.mean((mesh2.vertices[p2p] - mesh2.vertices[p2p_gt]) ** 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralZoomOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Error: tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "from model.neural_zoomout import NamFromP2pConverter, P2pFromNamConverter, NeuralZoomOut\n",
    "\n",
    "nam_from_p2p=NamFromP2pConverter()\n",
    "p2p_from_nam = P2pFromNamConverter()\n",
    "\n",
    "mesh1.basis.use_k=20\n",
    "mesh2.basis.use_k=20\n",
    "nam=nam_from_p2p(p2p_gt, mesh1.basis, mesh2.basis)\n",
    "\n",
    "\n",
    "mesh1.basis.use_k=200   \n",
    "mesh2.basis.use_k=200\n",
    "nzo = NeuralZoomOut(nit=9, step=20)\n",
    "ref_nam = nzo(nam, mesh1.basis, mesh2.basis)\n",
    "p2p = p2p_from_nam(ref_nam, mesh1.basis, mesh2.basis)\n",
    "print(\n",
    "    \"Conversion Error:\",\n",
    "    gs.mean((mesh2.vertices[p2p] - mesh2.vertices[p2p_gt]) ** 2),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
