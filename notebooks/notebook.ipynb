{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook on how to use NAM library for shape matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from model.neural_adjoint_map import *\n",
    "from model.optimizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import yaml\n",
    "sys.path.append('./../../NRP/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching/')\n",
    "import logging\n",
    "from os import path as osp\n",
    "\n",
    "from datasets import build_dataset\n",
    "from models import build_model\n",
    "\n",
    "import yaml\n",
    "from utils.tensor_util import to_device,to_numpy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#torch.cuda.set_device(0)\n",
    "\n",
    "#pointcloud\n",
    "\n",
    "DATASET='smal'\n",
    "\n",
    "with open('./../../NRP/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching/options/test/'+DATASET+'.yaml', 'r') as f:\n",
    "    opt = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "opt['is_train']=False\n",
    "test_set = build_dataset(opt['datasets']['test_dataset'])\n",
    "\n",
    "shape1=test_set[3]['first']\n",
    "shape2=test_set[3]['second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.linear_ref import *\n",
    "from methods.nam_ref import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load configuration\n",
    "DATASET = 'smal'\n",
    "\n",
    "test_set = build_dataset(opt['datasets']['test_dataset'])\n",
    "opt['is_train'] = False\n",
    "model = build_model(opt)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize variables\n",
    "n_exp = 20\n",
    "n_k = len(range(20, 201, 20)) + 1\n",
    "mse_nzo = np.zeros((n_exp, n_k))\n",
    "g_nzo = np.zeros((n_exp, n_k))\n",
    "\n",
    "#ind=np.random.randint(0,len(test_set),n_exp)\n",
    "ind=np.arange(n_exp)\n",
    "\n",
    "results = {}\n",
    "\n",
    "zo = []\n",
    "abzo =[]\n",
    "ima_l = []\n",
    "fsf_l = []\n",
    "\n",
    "n_zo = []\n",
    "n_ima = []\n",
    "n_fsf = []\n",
    "\n",
    "from torch_cluster import nearest\n",
    "\n",
    "\n",
    "for i in ind:\n",
    "    data_x, data_y = to_device(test_set[i]['second'], device), to_device(test_set[i]['first'], device)\n",
    "    if data_x['name'] != data_y['name']:\n",
    "        dist_y = data_y['dist'].cpu()\n",
    "        corr_x = data_x['corr']\n",
    "        corr_y = data_y['corr']\n",
    "        dist_y = to_numpy(dist_y)\n",
    "        corr_x = to_numpy(corr_x)\n",
    "        corr_y = to_numpy(corr_y)\n",
    "        C = torch.linalg.pinv(data_y['evecs'][corr_y, :20]) @ data_x['evecs'][corr_x, :20]\n",
    "        p2p_ini = nearest(data_x['evecs'][:, :20], data_y['evecs'][:, :20] @ C).cpu().numpy()\n",
    "\n",
    "        p2p=fsf(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        fsf_l.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=zoomout(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        zo.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=ima(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        ima_l.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=ab_zoomout(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        abzo.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=neural_ima(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        n_ima.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=neural_zoomout(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        n_zo.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "        p2p=neural_fsf(data_x['evecs'],data_y['evecs'],p2p_ini,20,201,20).cpu()\n",
    "        n_fsf.append(dist_y[p2p[corr_x],corr_y].mean())\n",
    "\n",
    "    i=i+1\n",
    "    print(i)\n",
    "print('fsf:',np.mean(fsf_l))\n",
    "print('zo:',np.mean(zo))\n",
    "print('ima:',np.mean(ima_l))\n",
    "print('abzo:',np.mean(abzo))\n",
    "\n",
    "print('n_fsf:',np.mean(n_fsf))\n",
    "print('n_zo:',np.mean(n_zo))\n",
    "print('n_ima:',np.mean(n_ima))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
